{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import nltk\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = \"../data/raw/text\"\n",
    "labels_dir =  \"../data/raw/labels\"\n",
    "audio_dir = \"../data/raw/audio\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(f\"{text_dir}/*.txt\")\n",
    "\n",
    "\n",
    "def remove_stamps_str(line)->str:\n",
    "    #clip_num = re.search('.+___\\d\\d?\\d?___.+', line).group(0)\n",
    "    stamp = re.search('.+___', line).group(0)\n",
    "    new_line = line.strip(stamp)\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncorpus = []\\ntext = []\\ntext_list = []\\nfor filename in text_files:\\n    text_str = ''\\n    with open(file=filename, encoding='utf-8') as f:\\n        lines = f.readlines()\\n        for count, line in enumerate(lines):\\n            clean_line = remove_stamps_str(line)\\n            text.append(clean_line)\\n    text_list.append(text)\\n    \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Pour python < 3.9, sinon str.removeprefix() de base\n",
    "def removeprefix(self: str, prefix: str, /) -> str:\n",
    "    if self.startswith(prefix):\n",
    "        return self[len(prefix):]\n",
    "    else:\n",
    "        return self[:]\n",
    "\n",
    "\n",
    "\n",
    "#Retire chaque stamps, chaque texte devien 1 seul str\n",
    "\n",
    "def text_list_generator(files_list):\n",
    "    text_list = []\n",
    "    for filename in files_list:\n",
    "        with open(file = filename, encoding = 'utf-8') as f:\n",
    "\n",
    "            ##WINDOWS SPECIFIC\n",
    "            if sys.platform == 'win32':\n",
    "           # videoid = filename.removeprefix(text_dir + '\\\\').rstrip('.txt')\n",
    "                videoid = removeprefix(filename, text_dir + '\\\\').rstrip('.txt')\n",
    "            else :\n",
    "                videoid = removeprefix(filename, text_dir + '/').rstrip('.txt')\n",
    "            lines = f.readlines()\n",
    "            for line_number, text_line in enumerate(lines):\n",
    "                clean_line = remove_stamps_str(text_line)\n",
    "                clip_id = videoid +'_'+ text_line.split('___')[1]\n",
    "                #clip_id = videoid +'_' +str(line_number)\n",
    "                yield (clip_id, clean_line.rstrip())\n",
    "\n",
    "\"\"\"\n",
    "corpus = []\n",
    "text = []\n",
    "text_list = []\n",
    "for filename in text_files:\n",
    "    text_str = ''\n",
    "    with open(file=filename, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for count, line in enumerate(lines):\n",
    "            clean_line = remove_stamps_str(line)\n",
    "            text.append(clean_line)\n",
    "    text_list.append(text)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "\n",
    "#création d'un dict pour lookup en O(1)\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "\n",
    "#Retire tous les timestamps en début de ligne, présents dans chaque transcript\n",
    "def remove_stamps_str(line)->str:\n",
    "    stamp = re.search('.+___', line).group(0)\n",
    "    new_line = line.strip(stamp)\n",
    "    return new_line\n",
    "\n",
    "#Retire les charactères non-ascii \n",
    "def remove_nonascii(line)->str:\n",
    "    ascii_line = line.encode(encoding = 'ascii', errors = 'ignore').decode()\n",
    "    return ascii_line\n",
    "\n",
    "#met tout en minuscules, retire les nombres et stopwords\n",
    "def clean_stopwords_digits(line)->str:\n",
    "    new_line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    new_line = ' '.join([word.lower() for word in new_line.split() if (len(word) >=2 and word.isalpha() and word not in stopwords_dict)])\n",
    "    return new_line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>happiness</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--qXJuDtHPw_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3g5yACwYnA_10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3g5yACwYnA_13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3g5yACwYnA_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3g5yACwYnA_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21813</th>\n",
       "      <td>zwTrXwi54us_6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21814</th>\n",
       "      <td>zwTrXwi54us_7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21815</th>\n",
       "      <td>zwTrXwi54us_8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21816</th>\n",
       "      <td>zwTrXwi54us_9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21817</th>\n",
       "      <td>zx4W0Vuus-I_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21818 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  anger  disgust  fear  happiness  sadness  surprise  \\\n",
       "0       --qXJuDtHPw_5      0        0     0          1        0         0   \n",
       "1      -3g5yACwYnA_10      0        0     0          0        0         0   \n",
       "2      -3g5yACwYnA_13      0        0     0          0        0         0   \n",
       "3       -3g5yACwYnA_2      0        0     0          1        0         0   \n",
       "4       -3g5yACwYnA_3      0        0     0          0        0         0   \n",
       "...               ...    ...      ...   ...        ...      ...       ...   \n",
       "21813   zwTrXwi54us_6      0        0     0          0        0         0   \n",
       "21814   zwTrXwi54us_7      0        0     0          0        0         0   \n",
       "21815   zwTrXwi54us_8      0        0     0          0        0         0   \n",
       "21816   zwTrXwi54us_9      0        0     0          0        0         0   \n",
       "21817   zx4W0Vuus-I_1      0        0     0          1        0         0   \n",
       "\n",
       "       sentiment  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "21813          0  \n",
       "21814          0  \n",
       "21815          0  \n",
       "21816          1  \n",
       "21817          1  \n",
       "\n",
       "[21818 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_df = pd.read_csv(\"../data/interim/labels/labels.csv\")\n",
    "display(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (text for text in text_list_generator(text_files))\n",
    "df_text = pd.DataFrame(corpus)\n",
    "df_text.columns = ['id', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--qXJuDtHPw_0</td>\n",
       "      <td>I see that there are three category of writers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--qXJuDtHPw_1</td>\n",
       "      <td>I define them as being an author, a writer, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--qXJuDtHPw_2</td>\n",
       "      <td>An author, I like to classify as somebody who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--qXJuDtHPw_3</td>\n",
       "      <td>These are the well-known authors of our time a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--qXJuDtHPw_4</td>\n",
       "      <td>Then, there is the writer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                               text\n",
       "0  --qXJuDtHPw_0    I see that there are three category of writers.\n",
       "1  --qXJuDtHPw_1  I define them as being an author, a writer, an...\n",
       "2  --qXJuDtHPw_2  An author, I like to classify as somebody who ...\n",
       "3  --qXJuDtHPw_3  These are the well-known authors of our time a...\n",
       "4  --qXJuDtHPw_4                         Then, there is the writer."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--qXJuDtHPw_0</td>\n",
       "      <td>I see that there are three category of writers.</td>\n",
       "      <td>see three category writers</td>\n",
       "      <td>[see, three, category, writers]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--qXJuDtHPw_1</td>\n",
       "      <td>I define them as being an author, a writer, an...</td>\n",
       "      <td>define author writer story teller</td>\n",
       "      <td>[define, author, writer, story, teller]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--qXJuDtHPw_2</td>\n",
       "      <td>An author, I like to classify as somebody who ...</td>\n",
       "      <td>an author like classify somebody writes great ...</td>\n",
       "      <td>[an, author, like, classify, somebody, writes,...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--qXJuDtHPw_3</td>\n",
       "      <td>These are the well-known authors of our time a...</td>\n",
       "      <td>these wellknown authors time past</td>\n",
       "      <td>[these, wellknown, authors, time, past]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--qXJuDtHPw_4</td>\n",
       "      <td>Then, there is the writer.</td>\n",
       "      <td>then writer</td>\n",
       "      <td>[then, writer]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44972</th>\n",
       "      <td>_ZlF5Q9W1DM_1</td>\n",
       "      <td>Clearly because everything is data driven toda...</td>\n",
       "      <td>clearly everything data driven today measurabl...</td>\n",
       "      <td>[clearly, everything, data, driven, today, mea...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44973</th>\n",
       "      <td>_ZlF5Q9W1DM_2</td>\n",
       "      <td>But the conceptual drivers, what sort of drive...</td>\n",
       "      <td>but conceptual drivers sort drives passions th...</td>\n",
       "      <td>[but, conceptual, drivers, sort, drives, passi...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44974</th>\n",
       "      <td>_ZlF5Q9W1DM_3</td>\n",
       "      <td>So those two things actually I think rub up ag...</td>\n",
       "      <td>so two things actually think rub collide reall...</td>\n",
       "      <td>[so, two, things, actually, think, rub, collid...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44975</th>\n",
       "      <td>_ZlF5Q9W1DM_4</td>\n",
       "      <td>So the idea of numbers telling the story up ag...</td>\n",
       "      <td>so idea numbers telling story idea explaining ...</td>\n",
       "      <td>[so, idea, numbers, telling, story, idea, expl...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44976</th>\n",
       "      <td>_ZlF5Q9W1DM_5</td>\n",
       "      <td>So I think those two things together are incre...</td>\n",
       "      <td>so think two things together incredibly import...</td>\n",
       "      <td>[so, think, two, things, together, incredibly,...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44977 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                               text  \\\n",
       "0      --qXJuDtHPw_0    I see that there are three category of writers.   \n",
       "1      --qXJuDtHPw_1  I define them as being an author, a writer, an...   \n",
       "2      --qXJuDtHPw_2  An author, I like to classify as somebody who ...   \n",
       "3      --qXJuDtHPw_3  These are the well-known authors of our time a...   \n",
       "4      --qXJuDtHPw_4                         Then, there is the writer.   \n",
       "...              ...                                                ...   \n",
       "44972  _ZlF5Q9W1DM_1  Clearly because everything is data driven toda...   \n",
       "44973  _ZlF5Q9W1DM_2  But the conceptual drivers, what sort of drive...   \n",
       "44974  _ZlF5Q9W1DM_3  So those two things actually I think rub up ag...   \n",
       "44975  _ZlF5Q9W1DM_4  So the idea of numbers telling the story up ag...   \n",
       "44976  _ZlF5Q9W1DM_5  So I think those two things together are incre...   \n",
       "\n",
       "                                              clean_text  \\\n",
       "0                             see three category writers   \n",
       "1                      define author writer story teller   \n",
       "2      an author like classify somebody writes great ...   \n",
       "3                      these wellknown authors time past   \n",
       "4                                            then writer   \n",
       "...                                                  ...   \n",
       "44972  clearly everything data driven today measurabl...   \n",
       "44973  but conceptual drivers sort drives passions th...   \n",
       "44974  so two things actually think rub collide reall...   \n",
       "44975  so idea numbers telling story idea explaining ...   \n",
       "44976  so think two things together incredibly import...   \n",
       "\n",
       "                                                  tokens  count  \n",
       "0                        [see, three, category, writers]      9  \n",
       "1                [define, author, writer, story, teller]     13  \n",
       "2      [an, author, like, classify, somebody, writes,...     14  \n",
       "3                [these, wellknown, authors, time, past]     11  \n",
       "4                                         [then, writer]      5  \n",
       "...                                                  ...    ...  \n",
       "44972  [clearly, everything, data, driven, today, mea...     17  \n",
       "44973  [but, conceptual, drivers, sort, drives, passi...     28  \n",
       "44974  [so, two, things, actually, think, rub, collid...     21  \n",
       "44975  [so, idea, numbers, telling, story, idea, expl...     40  \n",
       "44976  [so, think, two, things, together, incredibly,...     23  \n",
       "\n",
       "[44977 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_text['text'] =  df_text.text.apply(remove_nonascii)\n",
    "\n",
    "\n",
    "df_text['clean_text'] = df_text.text.apply(lambda s : clean_stopwords_digits(s))\n",
    "df_text['tokens'] = df_text.clean_text.apply(lambda x : x.split(' '))\n",
    "\n",
    "#df_text['lines'] = df_text.groupby('unique_id')\n",
    "df_text['count'] = df_text['text'].apply(lambda x : len(x.split(' ')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame = frame[~frame[target_features].sum(axis = 1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>count</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>happiness</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--qXJuDtHPw_5</td>\n",
       "      <td>I see that a writer is somebody who has an inc...</td>\n",
       "      <td>see writer somebody incredible command mechani...</td>\n",
       "      <td>[see, writer, somebody, incredible, command, m...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3g5yACwYnA_2</td>\n",
       "      <td>Key Polymer brings a technical aspect to our o...</td>\n",
       "      <td>key polymer brings technical aspect operation ...</td>\n",
       "      <td>[key, polymer, brings, technical, aspect, oper...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3g5yACwYnA_3</td>\n",
       "      <td>We're a huge user of adhesives for our operati...</td>\n",
       "      <td>were huge user adhesives operation called floc...</td>\n",
       "      <td>[were, huge, user, adhesives, operation, calle...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3g5yACwYnA_4</td>\n",
       "      <td>Key brings those types of aspects to a busines...</td>\n",
       "      <td>key brings types aspects business new markets ...</td>\n",
       "      <td>[key, brings, types, aspects, business, new, m...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3g5yACwYnA_9</td>\n",
       "      <td>We have many new opportunities through the way...</td>\n",
       "      <td>we many new opportunities way things changed y...</td>\n",
       "      <td>[we, many, new, opportunities, way, things, ch...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>_xjBqNfiEY4_6</td>\n",
       "      <td>And once again, U of I students are so well pr...</td>\n",
       "      <td>and students well prepared asset student</td>\n",
       "      <td>[and, students, well, prepared, asset, student]</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>_XWUKMkxa-Q_4</td>\n",
       "      <td>So its become more of an iterative process wit...</td>\n",
       "      <td>so become iterative process fast results fast ...</td>\n",
       "      <td>[so, become, iterative, process, fast, results...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>_YzIvwvyIq4_5</td>\n",
       "      <td>Secondly, using social, you know, things like ...</td>\n",
       "      <td>secondly using social know things like twitter...</td>\n",
       "      <td>[secondly, using, social, know, things, like, ...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>_ZlF5Q9W1DM_0</td>\n",
       "      <td>JOHN GERZEMA: When I think about finding insig...</td>\n",
       "      <td>john gerzema when think finding insights consu...</td>\n",
       "      <td>[john, gerzema, when, think, finding, insights...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>_ZlF5Q9W1DM_5</td>\n",
       "      <td>So I think those two things together are incre...</td>\n",
       "      <td>so think two things together incredibly import...</td>\n",
       "      <td>[so, think, two, things, together, incredibly,...</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21598 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                               text  \\\n",
       "0      --qXJuDtHPw_5  I see that a writer is somebody who has an inc...   \n",
       "1      -3g5yACwYnA_2  Key Polymer brings a technical aspect to our o...   \n",
       "2      -3g5yACwYnA_3  We're a huge user of adhesives for our operati...   \n",
       "3      -3g5yACwYnA_4  Key brings those types of aspects to a busines...   \n",
       "4      -3g5yACwYnA_9  We have many new opportunities through the way...   \n",
       "...              ...                                                ...   \n",
       "21593  _xjBqNfiEY4_6  And once again, U of I students are so well pr...   \n",
       "21594  _XWUKMkxa-Q_4  So its become more of an iterative process wit...   \n",
       "21595  _YzIvwvyIq4_5  Secondly, using social, you know, things like ...   \n",
       "21596  _ZlF5Q9W1DM_0  JOHN GERZEMA: When I think about finding insig...   \n",
       "21597  _ZlF5Q9W1DM_5  So I think those two things together are incre...   \n",
       "\n",
       "                                              clean_text  \\\n",
       "0      see writer somebody incredible command mechani...   \n",
       "1      key polymer brings technical aspect operation ...   \n",
       "2      were huge user adhesives operation called floc...   \n",
       "3      key brings types aspects business new markets ...   \n",
       "4      we many new opportunities way things changed y...   \n",
       "...                                                  ...   \n",
       "21593           and students well prepared asset student   \n",
       "21594  so become iterative process fast results fast ...   \n",
       "21595  secondly using social know things like twitter...   \n",
       "21596  john gerzema when think finding insights consu...   \n",
       "21597  so think two things together incredibly import...   \n",
       "\n",
       "                                                  tokens  count  anger  \\\n",
       "0      [see, writer, somebody, incredible, command, m...     18      0   \n",
       "1      [key, polymer, brings, technical, aspect, oper...     14      0   \n",
       "2      [were, huge, user, adhesives, operation, calle...     29      0   \n",
       "3      [key, brings, types, aspects, business, new, m...     34      0   \n",
       "4      [we, many, new, opportunities, way, things, ch...     21      0   \n",
       "...                                                  ...    ...    ...   \n",
       "21593    [and, students, well, prepared, asset, student]     24      0   \n",
       "21594  [so, become, iterative, process, fast, results...     13      0   \n",
       "21595  [secondly, using, social, know, things, like, ...     30      0   \n",
       "21596  [john, gerzema, when, think, finding, insights...     24      0   \n",
       "21597  [so, think, two, things, together, incredibly,...     23      0   \n",
       "\n",
       "       disgust  fear  happiness  sadness  surprise  sentiment  \n",
       "0            0     0          1        0         0          1  \n",
       "1            0     0          1        0         0          0  \n",
       "2            0     0          0        0         0          0  \n",
       "3            0     0          0        0         0          0  \n",
       "4            0     0          0        0         0          1  \n",
       "...        ...   ...        ...      ...       ...        ...  \n",
       "21593        0     0          0        0         0          0  \n",
       "21594        0     0          1        0         0          1  \n",
       "21595        0     0          1        0         0          1  \n",
       "21596        0     0          0        0         0          0  \n",
       "21597        0     0          0        0         0          0  \n",
       "\n",
       "[21598 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.merge(df_text, label_df, on = 'id', how = 'inner')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_emotions = ['anger', 'disgust', 'fear','happiness', 'sadness','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['len'] = frame.clean_text.apply(lambda x : len(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frame[frame[target_emotions].sum(axis=1) >0]\n",
    "\n",
    "frame = frame[~(frame['len'] > 30)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive_negative_frame : split all emotions into just 2 values, positive and negative.\n",
    "\n",
    "happiness + surprise = positive\n",
    "\n",
    "anger, disgust, fear, sadness = negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_negative_frame = frame.copy()\n",
    "positive_negative_frame = positive_negative_frame[positive_negative_frame[target_emotions].sum(axis=1) >= 1]\n",
    "\n",
    "positive_negative_frame = positive_negative_frame.drop(columns=['text','tokens', 'count', 'len', 'sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_or_neg(row):\n",
    "    if row['happiness'] or row['surprise']:\n",
    "        return 1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_negative_frame['pos'] = positive_negative_frame[target_emotions].apply(lambda row : pos_or_neg(row), axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6304\n",
       "0    2340\n",
       "Name: pos, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_negative_frame['pos'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_negative_frame = positive_negative_frame.groupby(by=['pos']).sample(1600, random_state = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#df_text['tagged'] = df_text['tokens'].apply(nltk.tag.pos_tag )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = tts(frame.clean_text, frame[target_emotions], test_size = 0.2)\\nX_train, X_test, y_train, y_test = data\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = tts(frame.clean_text, frame[target_emotions], test_size = 0.2)\n",
    "X_train, X_test, y_train, y_test = data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ncount_vector = CountVectorizer(lowercase=False, ngram_range=(1,3), max_features=250)\\ncount_vector.fit(frame.clean_text)\\ncounts = count_vector.transform(frame.clean_text)\\n\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "count_vector = CountVectorizer(lowercase=False, ngram_range=(1,3), max_features=250)\n",
    "count_vector.fit(frame.clean_text)\n",
    "counts = count_vector.transform(frame.clean_text)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntfidf_vector = TfidfVectorizer(lowercase=False, ngram_range=(1,3), max_features = 250)\\ntfidf_vector.fit(data[0], data[2])\\ntfidfs = tfidf_vector.transform(frame.clean_text)\\n\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tfidf_vector = TfidfVectorizer(lowercase=False, ngram_range=(1,3), max_features = 250)\n",
    "tfidf_vector.fit(data[0], data[2])\n",
    "tfidfs = tfidf_vector.transform(frame.clean_text)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnb_pipeline = Pipeline([\\n    ('BoW', count_vector),\\n    ('classifier', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior = None)))\\n])\\nfor feature in target_emotions:\\n    print(f'Fitting for {feature}')\\n    nb_pipeline.fit(X_train, y_train[feature])\\n    preds = nb_pipeline.predict(X_test)\\n    print(f'Test accuracy : {accuracy_score(y_test[feature], preds)}')\\n\\n\\nmod1 = OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior = None))\\n\\nmod1.fit(CountVectorizer.transform(X_train), y_train['happiness'])\\n\\n\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "nb_pipeline = Pipeline([\n",
    "    ('BoW', count_vector),\n",
    "    ('classifier', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior = None)))\n",
    "])\n",
    "for feature in target_emotions:\n",
    "    print(f'Fitting for {feature}')\n",
    "    nb_pipeline.fit(X_train, y_train[feature])\n",
    "    preds = nb_pipeline.predict(X_test)\n",
    "    print(f'Test accuracy : {accuracy_score(y_test[feature], preds)}')\n",
    "\n",
    "\n",
    "mod1 = OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior = None))\n",
    "\n",
    "mod1.fit(CountVectorizer.transform(X_train), y_train['happiness'])\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>happiness</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13788</th>\n",
       "      <td>JX-mwjSw0dk_0</td>\n",
       "      <td>work office lot paperwork administrative work ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11190</th>\n",
       "      <td>EO_5o9Gup6g_8</td>\n",
       "      <td>will congress convene lameduck session winter ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>28006_14</td>\n",
       "      <td>its horrible stupid movie</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18053</th>\n",
       "      <td>SZRxuS6fn9s_2</td>\n",
       "      <td>first people id like first address</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11941</th>\n",
       "      <td>g8Cl74tS3oY_3</td>\n",
       "      <td>its true federal support part funding necessar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15627</th>\n",
       "      <td>nw6Kf3AtCz4_26</td>\n",
       "      <td>be sure get questions using askfirebase social...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12039</th>\n",
       "      <td>gC_wF3uKFW0_4</td>\n",
       "      <td>so call us see wonderful</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12824</th>\n",
       "      <td>hThsntvCk2s_8</td>\n",
       "      <td>eck soon check often look forward hearing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>298774_5</td>\n",
       "      <td>and impressed film</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14247</th>\n",
       "      <td>KrmVX-HANew_5</td>\n",
       "      <td>ow want talk whether youre married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                         clean_text  \\\n",
       "13788   JX-mwjSw0dk_0  work office lot paperwork administrative work ...   \n",
       "11190   EO_5o9Gup6g_8  will congress convene lameduck session winter ...   \n",
       "5468         28006_14                          its horrible stupid movie   \n",
       "18053   SZRxuS6fn9s_2                 first people id like first address   \n",
       "11941   g8Cl74tS3oY_3  its true federal support part funding necessar...   \n",
       "...               ...                                                ...   \n",
       "15627  nw6Kf3AtCz4_26  be sure get questions using askfirebase social...   \n",
       "12039   gC_wF3uKFW0_4                           so call us see wonderful   \n",
       "12824   hThsntvCk2s_8          eck soon check often look forward hearing   \n",
       "5754         298774_5                                 and impressed film   \n",
       "14247   KrmVX-HANew_5                 ow want talk whether youre married   \n",
       "\n",
       "       anger  disgust  fear  happiness  sadness  surprise  pos  \n",
       "13788      0        0     0          0        1         0    0  \n",
       "11190      1        0     0          0        0         0    0  \n",
       "5468       1        1     0          0        0         0    0  \n",
       "18053      1        1     0          0        0         0    0  \n",
       "11941      0        0     0          0        1         0    0  \n",
       "...      ...      ...   ...        ...      ...       ...  ...  \n",
       "15627      0        0     0          1        0         0    1  \n",
       "12039      0        0     0          1        0         0    1  \n",
       "12824      0        0     0          1        0         0    1  \n",
       "5754       0        0     0          1        0         0    1  \n",
       "14247      0        0     0          1        0         0    1  \n",
       "\n",
       "[3200 rows x 9 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_negative_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(positive_negative_frame.clean_text.values, positive_negative_frame['pos'].values, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60\n",
    "\n",
    "def tokenize(data,max_len=MAX_LEN) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for i in range(len(data)):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            data[i],\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, train_masks = tokenize(X_train)\n",
    "test_ids, test_masks = tokenize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_model, max_len=MAX_LEN):\n",
    "    \n",
    "    ##params###\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n",
    "    loss = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "    \n",
    "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "    \n",
    "    embeddings = bert_model([input_ids,attention_masks])[1]\n",
    "    \n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(embeddings)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n",
    "    \n",
    "    model.compile(opt, loss=loss, metrics=['binary_accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 60)]         0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 60)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_9[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_10[0][0]']               \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 60,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            769         ['tf_bert_model_1[2][1]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,009\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(bert_model, MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9908405], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(bert_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "80/80 [==============================] - 351s 4s/step - loss: 0.4523 - binary_accuracy: 0.7988 - val_loss: 0.4896 - val_binary_accuracy: 0.7609\n",
      "Epoch 2/3\n",
      "80/80 [==============================] - 344s 4s/step - loss: 0.3657 - binary_accuracy: 0.8543 - val_loss: 0.5249 - val_binary_accuracy: 0.7656\n",
      "Epoch 3/3\n",
      "80/80 [==============================] - 371s 5s/step - loss: 0.2704 - binary_accuracy: 0.8984 - val_loss: 0.5650 - val_binary_accuracy: 0.7703\n"
     ]
    }
   ],
   "source": [
    "history_bert = model.fit([train_ids,train_masks], y_train, validation_data=([test_ids,test_masks], y_test), epochs=3, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 24s 1s/step\n"
     ]
    }
   ],
   "source": [
    "bert_results = model.predict([test_ids, test_masks])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.where(A > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results_enc = np.where(bert_results > 0.5, 1, 0)\n",
    "\n",
    "bert_results_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[225,  92],\n",
       "       [ 55, 268]], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true= y_test, y_pred=bert_results_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e016b4092f2f33b22d7afb9b895ac58d1e1c55d21f6dd8227b43a776f59f225e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
