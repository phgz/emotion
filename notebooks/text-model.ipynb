{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizerFast, TFBertModel\n",
    "from sklearn.model_selection import train_test_split as tts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = \"../data/raw/text\"\n",
    "labels_dir =  \"../data/raw/labels\"\n",
    "audio_dir = \"../data/raw/audio\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage, traitement etc\n",
    "Surtout utilisé pour prétraitement, mais une partie utile pour inférence de nouvelles données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(f\"{text_dir}/*.txt\")\n",
    "\n",
    "## Chaque ligne de texte est precede d'un \"stamp\" qui indique le clip de l'enregistrement etc\n",
    "def remove_stamps_str(line)->str:\n",
    "    #clip_num = re.search('.+___\\d\\d?\\d?___.+', line).group(0)\n",
    "    stamp = re.search('.+___', line).group(0)\n",
    "    new_line = line.strip(stamp)\n",
    "    return new_line\n",
    "\n",
    "## Pour python < 3.9, sinon str.removeprefix() de base\n",
    "def removeprefix(self: str, prefix: str, /) -> str:\n",
    "    if self.startswith(prefix):\n",
    "        return self[len(prefix):]\n",
    "    else:\n",
    "        return self[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def text_list_generator(files_list, text_dir):\n",
    "    text_list = []\n",
    "    for filename in files_list:\n",
    "        with open(file = filename, encoding = 'utf-8') as f:\n",
    "\n",
    "            ##WINDOWS SPECIFIC\n",
    "            if sys.platform == 'win32':\n",
    "                videoid = removeprefix(filename, text_dir + '\\\\').rstrip('.txt')\n",
    "            else :\n",
    "                videoid = removeprefix(filename, text_dir + '/').rstrip('.txt')\n",
    "            lines = f.readlines()\n",
    "            for line_number, text_line in enumerate(lines):\n",
    "                clean_line = remove_stamps_str(text_line)\n",
    "                clip_id = videoid +'_'+ text_line.split('___')[1]\n",
    "                #clip_id = videoid +'_' +str(line_number)\n",
    "                yield (clip_id, clean_line.rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retire tous les timestamps en début de ligne, présents dans chaque transcript\n",
    "def remove_stamps_str(line)->str:\n",
    "    stamp = re.search('.+___', line).group(0)\n",
    "    new_line = line.strip(stamp)\n",
    "    return new_line\n",
    "\n",
    "#Retire les charactères non-ascii \n",
    "def remove_nonascii(line)->str:\n",
    "    ascii_line = line.encode(encoding = 'ascii', errors = 'ignore').decode()\n",
    "    return ascii_line\n",
    "\n",
    "#met tout en minuscules, retire les nombres et stopwords\n",
    "def clean_punct_digits(line)->str:\n",
    "    new_line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    new_line = ' '.join([word.lower() for word in new_line.split() if (len(word) >=2 and word.isalpha())])\n",
    "    return new_line\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(\"../data/interim/labels/interim.csv\")\n",
    "display(label_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir des fichiers .txt, génère un dataframe qui contient une entrée pour chaque ligne de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (text for text in text_list_generator(text_files))\n",
    "df_text = pd.DataFrame(corpus)\n",
    "df_text.columns = ['id', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_text.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme le texte contient des charactères non-conformes (non ascii, autres langues etc), et de la mauvaise ponctuation,\n",
    "on nettoie chaque entrée en retirant ponctuation, nombres et non-ascii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text['text'] =  df_text.text.apply(remove_nonascii)\n",
    "df_text['clean_text'] = df_text.text.apply(lambda s : clean_punct_digits(s))\n",
    "\n",
    "display(df_text.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.merge(df_text, label_df, on = 'id', how = 'inner')\n",
    "display(frame.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un nouveau dataframe, \"new polarity\", pour n'utiliser que le texte nettoyé et la polarité du sentiment, de 0 pour négatif à 2 pour positif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_polarity_df = frame.copy()\n",
    "new_polarity_df.drop(columns=['id', 'text','anger', 'disgust','fear','happiness','sadness', 'surprise'], inplace= True)\n",
    "#Originalement, les valeurs sont de -1 à 1 mais modèle ne peut utiliser des valeurs négatives comme ça\n",
    "new_polarity_df.sentiment = new_polarity_df.sentiment.apply(lambda x : x+1)\n",
    "display(new_polarity_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_polarity_df = pd.read_csv('./polarity.csv')\n",
    "new_polarity_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        see that writer is somebody who has an incredi...\n",
       "1        key polymer brings technical aspect to our ope...\n",
       "2        were huge user of adhesives for our operation ...\n",
       "3        key brings those types of aspects to business ...\n",
       "4        we have many new opportunities through the way...\n",
       "                               ...                        \n",
       "15384    and once again of students are so well prepare...\n",
       "15385    so its become more of an iterative process wit...\n",
       "15386    secondly using social you know things like twi...\n",
       "15387    john gerzema when think about finding insights...\n",
       "15388    so think those two things together are incredi...\n",
       "Name: clean_text, Length: 15385, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_polarity_df.clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(new_polarity_df.sentiment.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données ne sont pas très bien balancées, mais la performance est bonne quand même. Comme on veut que ce soit \"réaliste\", et qu'il s'agit du plus gros dataset, on assume que c'est une représentation adéquate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle Tensorflow et BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.9999971   0.11710754 -0.9996999  -0.30514616 -0.9802392  -0.862202\n",
      "  -0.97073215 -0.695814    0.07994427 -0.13298516 -0.72146    -0.20102984\n",
      "   0.06001551  0.9999996  -0.34816775 -0.9673598   0.24690115  0.13613726\n",
      "  -0.86817634  0.9973174   0.88897586  0.03648714  0.99194896  0.78340924\n",
      "  -0.99999815 -0.11478306 -0.9996667   0.61504483  0.9986014   0.0180592\n",
      "  -0.10286351  0.03739744 -0.99590176 -0.9641891   0.75952166  0.99997634\n",
      "  -0.97950923 -0.00245375  0.999005   -0.9982019   0.9824466   0.97204584\n",
      "  -0.99923104  0.9809128  -0.9996157  -0.17666654 -0.99808425  0.9996697\n",
      "   0.97371936  0.99670094  0.7636754  -0.7076082  -0.1536308  -0.27309185\n",
      "   0.995362    0.98568845 -0.6754971  -0.7342357   0.9929015  -0.7974375\n",
      "  -0.01379627  0.85045284 -0.9929437   0.98595965 -0.8722031  -0.9999986\n",
      "  -0.15873732  0.97247326  0.9354089   0.98516846  0.9922101   0.17193754\n",
      "  -0.99890375  0.04214636  0.5715292  -0.9942668  -0.8313425   0.13212052\n",
      "  -0.36228302  0.07027563  0.32081488 -0.0542865  -0.9918888  -0.9998827\n",
      "   0.9999266  -0.9913658   0.41925266  0.12492132 -0.53969634  0.6569778\n",
      "   0.01759405  0.97739744 -0.6408375   0.9667955   0.71213984  0.66351867\n",
      "  -0.65469944  0.9767467  -0.9998195  -0.9851192  -0.99558985  0.94217247\n",
      "  -0.9997421  -0.9691668  -0.9944295  -0.6909114  -0.99742836 -0.99415785\n",
      "   0.7183039   0.85464835  0.999396    0.34399268 -0.85292226  0.9992231\n",
      "  -0.99999386 -0.01484865  0.19828379  0.48578852  0.29445562 -0.9928837\n",
      "   0.23129492 -0.99999297 -0.5982834   0.98486596 -0.9997811   0.983139\n",
      "   0.99794805  0.98119926]], shape=(1, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "module_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
    "BERT_LAYER = hub.KerasLayer(module_url, trainable=True)\n",
    "MAX_LEN = 256\n",
    "seq_length = 128\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\",\n",
    "    trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 128].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 128].\n",
    "\n",
    "embedding_model = tf.keras.Model(text_input, pooled_output)\n",
    "sentences = tf.constant([\"(your text here)\"])\n",
    "print(embedding_model(sentences))\n",
    "\n",
    "encoder_inputs = dict(\n",
    "    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),\n",
    "    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),\n",
    "    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"keras_layer_25\" (type KerasLayer).\n\nin user code:\n\n    File \"c:\\Users\\trott\\ml_env\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 237, in call  *\n        result = smart_cond.smart_cond(training,\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * [<tf.Tensor 'inputs:0' shape=(None, 20) dtype=int32>,\n     <tf.Tensor 'inputs_1:0' shape=(None, 20) dtype=int32>,\n     <tf.Tensor 'inputs_2:0' shape=(None, 20) dtype=int32>]\n        * False\n        * None\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'),\n     'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'),\n     'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids')}\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'),\n     'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'),\n     'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'),\n     'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'),\n     'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'),\n     'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'),\n     'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids')}\n        * True\n        * None\n      Keyword arguments: {}\n\n\nCall arguments received by layer \"keras_layer_25\" (type KerasLayer):\n  • inputs=['tf.Tensor(shape=(None, 20), dtype=int32)', 'tf.Tensor(shape=(None, 20), dtype=int32)', 'tf.Tensor(shape=(None, 20), dtype=int32)']\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Projet_emotion\\emotion\\notebooks\\text-model.ipynb Cell 24\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projet_emotion/emotion/notebooks/text-model.ipynb#ch0000023?line=87'>88</a>\u001b[0m y \u001b[39m=\u001b[39m dummy_sents\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projet_emotion/emotion/notebooks/text-model.ipynb#ch0000023?line=88'>89</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m tts(x, y, test_size \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projet_emotion/emotion/notebooks/text-model.ipynb#ch0000023?line=89'>90</a>\u001b[0m model2 \u001b[39m=\u001b[39m build_model()\n",
      "\u001b[1;32me:\\Projet_emotion\\emotion\\notebooks\\text-model.ipynb Cell 24\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(bert_layer, max_len)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projet_emotion/emotion/notebooks/text-model.ipynb#ch0000023?line=47'>48</a>\u001b[0m input_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(max_len,), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_mask\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projet_emotion/emotion/notebooks/text-model.ipynb#ch0000023?line=48'>49</a>\u001b[0m segment_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(max_len,), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msegment_ids\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projet_emotion/emotion/notebooks/text-model.ipynb#ch0000023?line=50'>51</a>\u001b[0m pooled_output, sequence_output \u001b[39m=\u001b[39m bert_layer([input_word_ids, input_mask, segment_ids])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projet_emotion/emotion/notebooks/text-model.ipynb#ch0000023?line=51'>52</a>\u001b[0m clf_output \u001b[39m=\u001b[39m sequence_output[:, \u001b[39m0\u001b[39m, :]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projet_emotion/emotion/notebooks/text-model.ipynb#ch0000023?line=53'>54</a>\u001b[0m net \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m16\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(clf_output)\n",
      "File \u001b[1;32mc:\\Users\\trott\\ml_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2g05mzpb.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     72\u001b[0m     result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(smart_cond)\u001b[39m.\u001b[39msmart_cond, (ag__\u001b[39m.\u001b[39mld(training), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), fscope))), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     73\u001b[0m result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_6\u001b[39m():\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m (result,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2g05mzpb.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     71\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(smart_cond)\u001b[39m.\u001b[39;49msmart_cond, (ag__\u001b[39m.\u001b[39;49mld(training), ag__\u001b[39m.\u001b[39;49mautograph_artifact(\u001b[39mlambda\u001b[39;49;00m : ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(f), (), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), fscope)), ag__\u001b[39m.\u001b[39;49mautograph_artifact(\u001b[39mlambda\u001b[39;49;00m : ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(f), (), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m), fscope))), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2g05mzpb.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     71\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(smart_cond)\u001b[39m.\u001b[39msmart_cond, (ag__\u001b[39m.\u001b[39mld(training), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), fscope))), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"keras_layer_25\" (type KerasLayer).\n\nin user code:\n\n    File \"c:\\Users\\trott\\ml_env\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 237, in call  *\n        result = smart_cond.smart_cond(training,\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * [<tf.Tensor 'inputs:0' shape=(None, 20) dtype=int32>,\n     <tf.Tensor 'inputs_1:0' shape=(None, 20) dtype=int32>,\n     <tf.Tensor 'inputs_2:0' shape=(None, 20) dtype=int32>]\n        * False\n        * None\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'),\n     'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'),\n     'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids')}\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'),\n     'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'),\n     'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'),\n     'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'),\n     'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'),\n     'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'),\n     'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids')}\n        * True\n        * None\n      Keyword arguments: {}\n\n\nCall arguments received by layer \"keras_layer_25\" (type KerasLayer):\n  • inputs=['tf.Tensor(shape=(None, 20), dtype=int32)', 'tf.Tensor(shape=(None, 20), dtype=int32)', 'tf.Tensor(shape=(None, 20), dtype=int32)']\n  • training=False"
     ]
    }
   ],
   "source": [
    "## Comme un NN avec BERT est extrêmement lourd (surtout pour HEROKU), très petit MAX_LEN pour réduire la taille des vecteurs et donc le nombre de paramètres\n",
    "MAX_LEN = 20\n",
    "\n",
    "#Le modele pre entraine BERT est utilise sous forme dun HubLayer \n",
    "#module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "module_url = 'https://tfhub.dev/google/tn_bert/1'\n",
    "#module_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
    "BERT_LAYER = hub.KerasLayer(module_url, trainable=False)\n",
    "#Tokenizer qui prend les strings propres et calcul les embeddings\n",
    "TOKENIZER = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "#Quelques metriques intéressantes\n",
    "#Bien que F1_score serait interessant, KERAS  ne le fourni pas car ce nest pas une bonne metrique calculee par batch\n",
    "METRICS = [\n",
    "    tf.keras.metrics.CategoricalAccuracy(name = 'accuracy'),\n",
    "    tf.keras.metrics.Precision(name = 'precision'),\n",
    "    tf.keras.metrics.Recall(name = 'recall')\n",
    "]\n",
    "\n",
    "#Encodes lists of texts into BERT-useable tensors\n",
    "def bert_encode(texts, tokenizer=TOKENIZER, max_len=MAX_LEN):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "\n",
    "#Building a neural-net that uses BERT embeddings\n",
    "def build_model(bert_layer=BERT_LAYER, max_len=MAX_LEN):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "\n",
    "    net = tf.keras.layers.Dense(16, activation='relu')(clf_output)\n",
    "    net = tf.keras.layers.Dropout(0.9)(net)\n",
    "    out = tf.keras.layers.Dense(3, activation='softmax')(net)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08), loss='categorical_crossentropy', metrics=METRICS)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_dnn(model, X_train, y_train, e=1):\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('smaller_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "\n",
    "    train_input = bert_encode(X_train)\n",
    "    train_labels = y_train\n",
    "\n",
    "    train_history = model.fit(\n",
    "            train_input, train_labels, \n",
    "            validation_split=0.2,\n",
    "            epochs=e,\n",
    "            callbacks=[checkpoint, earlystopping],\n",
    "            batch_size=4,\n",
    "            verbose=1\n",
    "            )\n",
    "    return train_history\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#data = pd.read_csv('polarity_balanced.csv')\n",
    "data = new_polarity_df\n",
    "x = data.clean_text.values\n",
    "dummy_sents = pd.get_dummies(data.sentiment)\n",
    "y = dummy_sents.values\n",
    "X_train, X_test, y_train, y_test = tts(x, y, test_size = 0.1)\n",
    "model2 = build_model()\n",
    "\n",
    "#histo = train_dnn(model2, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model2.to_json()\n",
    "with open(\"text_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model2.save_weights(f\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
    "BERT_LAYER = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "def build_model(bert_layer=BERT_LAYER, max_len=MAX_LEN):\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "    preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "    encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\",\n",
    "    trainable=True)\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    pooled_output = outputs[\"pooled_output\"]      # [batch_size, 128].\n",
    "    sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 128].\n",
    "    embedding_model = tf.keras.Model(text_input, pooled_output)\n",
    "        \n",
    "    return embedding_model\n",
    "mini_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(histo.history['accuracy'])\n",
    "plt.plot(histo.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(histo.history['loss'])\n",
    "plt.plot(histo.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(histo.history['recall'])\n",
    "plt.plot(histo.history['val_recall'])\n",
    "plt.title('model recall')\n",
    "plt.ylabel('recall')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X = bert_encode(X_test)\n",
    "preds = loaded_model.predict(encoded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\"}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_true, true_preds, display_labels = ['negative', 'neutral', 'positive'])\n",
    "plt.title('Text model predictions', color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true = pd.Series([np.argmax(y) for y in y_test])\n",
    "true_preds = [np.argmax(x) for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics_per_class(y_test_true, true_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e016b4092f2f33b22d7afb9b895ac58d1e1c55d21f6dd8227b43a776f59f225e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
